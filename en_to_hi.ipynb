{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/MILDSum_test_468.zip -d /content/drive/MyDrive/MILDSum_test"
      ],
      "metadata": {
        "id": "waKA3mfWBjUR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/MILDSum_train_2185.zip -d /content/drive/MyDrive/MILDSum_train"
      ],
      "metadata": {
        "id": "chSXutgwCD-G",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/drive/MyDrive/MILDSum_val_469.zip -d /content/drive/MyDrive/MILDSum_val"
      ],
      "metadata": {
        "id": "PMPCEKxwCNPz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install performer-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx6dlTQMGnw5",
        "outputId": "7a019f8c-28b5-4232-b622-9d66b2618a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"update\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-fast-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Owz5rj-cvUD",
        "outputId": "67638e04-e040-4ed8-9caa-0f3c45ac8206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fast-transformers\n",
            "  Downloading pytorch-fast-transformers-0.4.0.tar.gz (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch-fast-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-fast-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch-fast-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch-fast-transformers) (3.0.2)\n",
            "Building wheels for collected packages: pytorch-fast-transformers\n",
            "  Building wheel for pytorch-fast-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fast-transformers: filename=pytorch_fast_transformers-0.4.0-cp311-cp311-linux_x86_64.whl size=21343742 sha256=073235c85272f9c1f64324f0195476ff0e67f9cbd1fc9d1c8d904af39a4a531d\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/0c/7d/39fe4c512d9a96aac3384d1506af78b0955e71e79163e459ec\n",
            "Successfully built pytorch-fast-transformers\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9XEj9nuAyK8",
        "outputId": "d01880db-da35-4c13-d7f1-b9089b37c428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Samples: ['The Calcutta High Court has recently quashed criminal proceedings initiated against a lawyer for providing allegedly false and improper legal advice which was instrumental in sanction of a bank loan to a company which was later declared as Non Performing Asset (NPA) with an outstanding due of Rs. 2.57 crores. \\nJustice Ananda Kumar Mukherjee observed that merely because the lawyer\\'s opinion was not acceptable, criminal proceedings cannot be initiated against him especially in the absence of any tangible evidence that he was associated with other conspirators.\\n\"..it is beyond doubt that a lawyer owes an \"unremitting loyalty\" to the interests of the client and it is the lawyer\\'s responsibility to act in a manner that would best advance the interest of the client. Merely because his opinion may not be acceptable, he cannot be mulcted with the criminal prosecution, particularly, in the absence of tangible evidence that he associated with other conspirators. At the most, he may be liable for gross negligence or professional misconduct if it is established by acceptable evidence and cannot be charged for the offence under sections 420 and 109 IPC along with other conspirators without proper and acceptable link between them\", the Court observed. \\nThe Court further opined that if there is a link for evidence to connect him with the other conspirators for causing loss to the institution undoubtedly, the prosecuting authorities are entitled to proceed under criminal prosecution however such tangible evidence is absent in the instant case. \\nIn the instant case, the lawyer had been fastened with criminal liability under sections 420, 468, 471, 120B of the IPC. Thereafter, he had filed an application for quashing of the criminal proceedings initiated by the Central Bureau of Investigation, Enforcement Offences Wing, Kolkata before the concerned Metropolitan Magistrate. \\nUpon perusal of the record, the Court noted that the petitioner had not been named in the FIR and that if the complaint is taken on its face value and accepted in their entirety, no offence is constituted against the petitioner to make out a case against him. It was further opined that no material has surfaced in course of investigation to establish any manner of connection and involvement of the petitioner with the principal accused in defrauding the bank to disburse the loan to the company.\\nIt was also noted that there is no allegation or material in the charge sheet that the petitioner made any wrongful gain from the co-accused persons or he had any pecuniary benefit for preparing a wrong search report in favour of the company.\\nThe Court further observed that it is undisputed that the loan was sanctioned in favour of the accused company and that the title deeds and other fabricated and forged documents relating to the property were handed over to the concerned bank on arrangement made by the principal accused person, including the directors of the company who had applied for the loan.\\nOpining that the petitioner should have been more careful in ascertaining the genuineness of the title deeds, the Court remarked, \\n\"The petitioner considered such false and forged documents and prepared his report. He should have been more cautious and careful to hold search at the A.D.S.R office to ascertain if the Title Deeds where entered in the Register to determine ownership of the property and the genuineness of the Deeds and also find out the genuineness of the record of rights from the Land Revenue Office to unearth the names of recorded owner and the nature of the land. The petitioner appears to have been negligent by not fulfilling his duties in proper manner.\"\\nHowever, the Court observed that the report prepared by the petitioner does not lead to any presumption that the petitioner had any connection with the actual beneficiaries for releasing of the loan. It was further stated that the allegations made in the complaint and the evidence collected against the petitioner in support of the same during investigation do not prima facie constitute commission of any offence of defrauding the bank by the petitioner.\\nReliance was also placed on the Supreme Court decision in Central Bureau of Investigation, Hydrabad v. K. Narayana Rao wherein the Apex Court had inter alia observed that criminal prosecution on the basis of such bald and omnibus allegations against the panel advocates of the Bank ought not to be allowed to proceed as the same constitutes an abuse of the process of the court and such prosecution may in all likelihood be abortive and futile.\\nFurthermore, the Delhi High Court judgment in A. Kumar Sharma v. CBI was also relied upon wherein the High Court had quashed the proceeding against an panel advocate who had submitted a false search report in alleged collusion with the principal accused, holding therein that mere negligence or want of greater professional care and competence on the part of an advocate would not make him liable for a criminal offence in absence of tangible evidence.\\nAccordingly, the Court set aside the criminal proceedings against the petitioner by observing, \\n\" I find and have no hesitation to hold that there is no tangible evidence to establish any connection or collusion between the petitioner and other co- accused persons for the purpose of defrauding the bank in sanction of the loan amount. Even there is no iota of evidence in the charge sheet where from it would indicate that the petitioner due to such act had made a wrongful gain from the other beneficiaries. Under such circumstances continuation of this proceeding against the petitioner would amount to an abuse of the process of court\"', 'The Andhra Pradesh High Court has reiterated the established principle that even if there is an alternative remedy available, in case the rules of natural justice have not been followed, the aggrieved may approach the High Court by way of a writ petition under Article 226 of the Constitution.\\n The Petitioners in this case had applied for mutation of their names to the land purchased by them. The contention of the Petitioners was that the said application was rejected without following the procedure stipulated under AP Rights in Land and Pattadar Passbooks Act, 1971, specifically Section 5 of the Act which provides that the petitioner should have been given a notice and thereafter the order should have been passed after giving the Petitioner an opportunity of hearing. \\nIt was argued on behalf of the Petitioners that such an exercise was not followed.\\n On behalf of the Respondent Authorities it was argued that no further order is required from the authorities and the only right available to the Petitioners was to file an appeal.\\n At the outset, the court observed that although Section 5 mandates that an order be passed only after issuing notice to the parties and giving them an opportunity of hearing, the impugned order did not meet the criteria. \\n\"A prima facie reading of the order shows that no notice was issued to the petitioners. Only two documents have been referred to in the order i.e., the mutation application made by the petitioner No.3 and the enquiry report of the Mandal Revenue Inspector. Beyond this there is no reference to any notice being issued etc.,\" the Court noted.\\nHence, it was of the opinion that the respondent must conduct a de novo enquiry into the application filed by the petitioners, strictly in compliance with the provisions of Section 5 of the Act and Rules. there under.\\nIt observed,\\n\"Keeping this writ petition pending is therefore is not called for. Once there is a failure of rules of natural justice, even if there is an alternative remedy, a writ is maintainable. The law is well settled.\"\\nResultantly, the High Court set aside the order of rejection and directed the respondents to follow the procedure under law and pass a fresh order,'] ['कलकत्ता हाईकोर्ट ने हाल ही में एक वकील के खिलाफ कथित रूप से गलत और अनुचित कानूनी सलाह देने के आरोप में शुरू की गई आपराधिक कार्यवाही को रद्द कर दिया। उक्त कानूनी सलाह एक कंपनी को बैंक ऋण स्वीकृत कराने में सहायक थी, जिसे बाद में गैर-निष्पादित संपत्ति (एनपीए) घोषित कर दिया गया। लोन की बकाया राशि 2.57 करोड़ रुपये थी।\\nजस्टिस आनंद कुमार मुखर्जी ने कहा कि केवल इसलिए कि वकील की राय स्वीकार्य नहीं थी, उसके खिलाफ आपराधिक कार्यवाही शुरू नहीं की जा सकती, विशेष रूप से किसी भी ठोस सबूत के अभाव में कि वह अन्य साजिशकर्ताओं से जुड़ा था।\\nकोर्ट ने कहा,\\n\"इसमें कोई संदेह नहीं है कि एक वकील अपने मुवक्किल के हितों के लिए \"निरंतर वफादारी\" रखता है और उसकी जिम्मेदारी है कि वह ऐसे कार्य करे जो मुव\\u200cक्किल के हित को सर्वोत्तम रूप से आगे बढ़ाए। केवल इसलिए कि उसकी राय स्वीकार्य नहीं हो सकती है, उस पर आपराधिक मुकदमा नहीं चलाया जा सकता, विशेष रूप से ठोस सबूत के अभाव में कि वह अन्य साजिशकर्ताओं से जुड़ा था। वह घोर लापरवाही या पेशेवर कदाचार के लिए उत्तरदायी हो सकता है, यदि यह स्वीकार्य साक्ष्य द्वारा स्थापित किया जाता है, हालांकि उसे अन्य साजिशकर्ताओं के साथ आईपीसी की धारा 420 और 109 के तहत, साजिशकर्ताओं और उसके बीच उचित और स्वीकार्य लिंक के बिना, अपराध के लिए आरोपित नहीं किया जा सकता है।\"\\nअदालत ने आगे कहा कि अगर सबूत के लिए अन्य साजिशकर्ताओं के साथ उसे जोड़ने के लिए लिंक है तो निस्संदेह संस्था को नुकसान पहुंचाने के लिए आपराधिक अभियोजन के तहत आगे बढ़ा जा सकता है, हालांकि इस तरह के ठोस सबूत मौजूदा मामले में अनुपस्थित हैं।\\nइस मामले में वकील पर आईपीसी की धारा 420, 468, 471 और 120बी के तहत आपराधिक दायित्व तय किया गया है, जिसके बाद उन्होंने संबंधित मेट्रोपॉलिटन मजिस्ट्रेट के समक्ष सीबाआई, इनफोर्समेंट ऑफेंस विंग, कोलकाता द्वारा शुरू की गई आपराधिक कार्यवाही को रद्द करने के लिए एक आवेदन दायर किया था।\\nरिकॉर्ड को देखने के बाद कोर्ट ने कहा कि याचिकाकर्ता का नाम एफआईआर में नहीं था और अगर शिकायत को उसकी फेस वैल्यू पर लिया जाता है और उसकी संपूर्णता में स्वीकार किया जाता है, तो मामला बनाने के लिए याचिकाकर्ता के खिलाफ कोई अपराध नहीं बनता है।\\nकोर्ट ने आगे कहा कि जांच में ऐसी कोई सामग्री सामने नहीं आई, जिससे कंपनी को ऋण देने के लिए बैंक को धोखा देने में मुख्य आरोपी के साथ याचिकाकर्ता के किसी भी तरह के संबंध और संलिप्तता को स्थापित किया जा सके।\\nयह भी नोट किया गया कि चार्जशीट में ऐसा कोई आरोप या सामग्री नहीं है कि याचिकाकर्ता ने सह-आरोपी व्यक्तियों से कोई गलत लाभ कमाया या कंपनी के पक्ष में झूठी सर्च रिपोर्ट तैयार करने पर उसे कोई आर्थिक लाभ हुआ।\\nअदालत ने आगे कहा कि यह निर्विवाद है कि ऋण आरोपी कंपनी के पक्ष में स्वीकृत किया गया था और संपत्ति से संबंधित टाइटल डीड्स और अन्य जाली दस्तावेज संबंधित बैंक को मुख्य आरोपी व्यक्ति ने दिलवाए थे। इसमें कंपनी के निदेशक भी शामिल थे, जिन्होंने ऋण के लिए आवेदन किया था।\\nकोर्ट ने यह माना कि याचिकाकर्ता को टाइटल डीड्स की वास्तविकता का पता लगाने में अधिक सावधानी बरतनी चाहिए थी। हालांकि, आगे कहा कि याचिकाकर्ता द्वारा तैयार की गई रिपोर्ट से यह अनुमान नहीं लगता है कि याचिकाकर्ता का ऋण जारी करने के लिए वास्तविक लाभार्थियों के साथ कोई संबंध था।\\nकोर्ट ने आगे यह भी कहा कि लगाए गए आरोप और जांच में याचिकाकर्ता के खिलाफ जुटाए गए सबूत प्रथम दृष्टया बैंक को धोखा देने के किसी भी अपराध का गठन नहीं करते हैं।\\nअदालत ने याचिकाकर्ता के खिलाफ आपराधिक कार्यवाही रद्द करते हुए कहा, \"मुझे यह मानने में कोई हिचकिचाहट नहीं है कि..याचिकाकर्ता और अन्य सह-आरोप\\u200cियों के बीच कोई संबंध या मिलीभगत स्थापित करने के लिए ठोस सबूत नहीं है।\"\\nसिटेशन: 2022 लाइव लॉ (Cal) 165', 'आंध्र प्रदेश हाईकोर्ट ने दोहराया कि प्राकृतिक न्याय के नियमों का पालन नहीं करने पर भले ही कोई वैकल्पिक उपाय उपलब्ध हो, तो पीड़ित व्यक्ति संविधान के अनुच्छेद 226 के तहत एक रिट याचिका के माध्यम से हाईकोर्ट का दरवाजा खटखटा सकता है।\\nइस मामले में याचिकाकर्ताओं ने उसके द्वारा खरीदी गई जमीन पर अपना नाम बदलने के लिए आवेदन किया था। याचिकाकर्ताओं का तर्क था कि उक्त आवेदन भूमि और पट्टादार पासबुक अधिनियम, 1971 में एपी राइट्स, विशेष रूप से अधिनियम की धारा पांच के तहत निर्धारित प्रक्रिया का पालन किए बिना खारिज कर दिया गया। इस अधिनियम में प्रावधान है कि याचिकाकर्ता को नोटिस दिया जाना चाहिए था। उसके बाद आदेश याचिकाकर्ता को सुनवाई का अवसर देने के बाद पारित किया जाना चाहिए था।\\nयाचिकाकर्ताओं की ओर से यह तर्क दिया गया कि इस तरह की प्रक्रिया का पालन नहीं किया गया।\\nप्रतिवादी प्राधिकारियों की ओर से यह तर्क दिया गया कि प्राधिकारियों से किसी और आदेश की आवश्यकता नहीं और याचिकाकर्ताओं के पास अपील दायर करने का एकमात्र अधिकार उपलब्ध था।\\nशुरुआत में अदालत ने कहा कि हालांकि धारा पांच में कहा गया कि पक्षकारों को नोटिस जारी करने और उन्हें सुनवाई का मौका देने के बाद ही आदेश पारित किया जाना चाहिए, लेकिन आक्षेपित आदेश मानदंडों को पूरा नहीं करता।\\nकोर्ट ने कहा,\\n\"आदेश को प्रथम दृष्टया पढ़ने से पता चलता है कि याचिकाकर्ताओं को कोई नोटिस जारी नहीं किया गया। आदेश में केवल दो दस्तावेजों का उल्लेख किया गया। इनमें याचिकाकर्ता नंबर तीन द्वारा किए गए उत्परिवर्तन आवेदन और मंडल राजस्व निरीक्षक की जांच रिपोर्ट शामिल है। इसके अलावा किसी नोटिस आदि जारी करने का कोई उल्लेख नहीं है।\"\\nइसलिए, यह राय है कि प्रतिवादी को याचिकाकर्ताओं द्वारा दायर आवेदन में अधिनियम और नियमों की धारा पांच के प्रावधानों का कड़ाई से अनुपालन करते हुए नए सिरे से जांच करनी चाहिए।\\nकोर्ट ने यह भी कहा,\\n\"इसलिए इस रिट याचिका को लंबित रखने की आवश्यकता नहीं है। एक बार प्राकृतिक न्याय के नियमों की विफलता होने पर भले ही कोई वैकल्पिक उपाय हो, एक रिट याचिका सुनवाई योग्य है। इस संबंध में कानून पूरी तरह स्पष्ट है।\"\\nपरिणामस्वरूप, हाईकोर्ट ने अस्वीकृति के आदेश को रद्द कर दिया और प्रतिवादियों को कानून के तहत प्रक्रिया का पालन करने और एक नया आदेश पारित करने का निर्देश दिया।']\n",
            "Test Data Samples: ['The Allahabad High Court recently observed that a first information report under the Uttar Pradesh Gangsters and Anti-Social Activities (Prevention) Act, 1986 can be lodged on the basis of the involvement of an accused in a single previous case\\nThe Bench of Justice Surya Prakash Kesarwani and Justice Piyush Agrawal observed thus while relying upon an earlier judgment of the High Court in the case of Ritesh Kumar Alias Rikki vs. State of U.P. and another.\\nIn the Ritesh Kumar case, the HC had held that the lodging of a first information report under the UP Gangsters Act even on the basis of the involvement of a person in a single case, is valid and permissible.\\nIn the instant case before the Allahabad HC, one Anwar Shahzad had moved the Court seeking to quash an FIR registered against him under section 3 (1) of the Gangster & Anti Social Activities Act 1986.\\nIt was contended by him that in the impugned FIR, there is a mention of a Case against him under Sections 419, 420, 467, 468, 471, 120-B I.P.C., and Section 30 Arms Act. He further argued that he is innocent and has been falsely implicated because he is the real brother-in-law of Mukhtar Ansari.\\nIt was further stated that the present government has started a policy to harass the political opponent who had fought and won MLA/ parliamentary elections against the ruling party candidate.\\nOn the other hand, the AGA submitted that the petitioner is a member of the gang, and habitual of committing crimes as provided under Section 2(b) of the Act, 1986. \\nTherefore, it was argued, that the registration of the impugned First Information Report on the basis of materials available with the police satisfies the ingredients of gangsters and the penal clause i.e. Section 3 (1) of the Act, 1986, and thus, fully justifies registration of the impugned First Information Report.\\nAgainst this backdrop, the Court, while stressing that the criminal proceedings ought not to be scuttled at the initial stage and that quashing of complaint/FIR should be an exception rather than an ordinary rule, the Court opined thus:\\n\"The power of quashing should be exercised sparingly with circumspection, in the rarest of rare cases. While examining an FIR/complaint, quashing of which is sought, the Court cannot embark upon an enquiry as to the reliability or genuineness or otherwise of the allegations made in the FIR/complaint...Ordinarily, the Courts are barred from usurping the jurisdiction of the police, since the two organs of the State operate in two specific spheres of activities and one ought not to tread over the other sphere. The First Information Report is not an encyclopedia that must disclose all facts and details regarding the offence reported. Therefore, when the investigation by the police is in progress, the Court should not go into merits of the allegations made in the FIR. Police must be permitted to complete the investigation.\"\\nIn view of this observation, the Court dismissed the plea.\\nIn related news, last month the Supreme Court also observed that there can be a prosecution against a person under the Uttar Pradesh Gangsters and Anti-Social Activities (Prevention) Act, 1986, even in the case of a single offence/FIR/charge-sheet for any of the anti-social activities mentioned in section 2(b) of the Act.', 'The Bombay High Court recently allowed the termination of a 16 weeks\\' pregnancy of a minor who was a victim of sexual abuse and was also in custody at an Observation Home for a crime of murder under section 302 of the Indian Penal Code (IPC).\\n A bench of Justices A.S. Chandurkar and Urmila Joshi-Phalke noted that the Apex Court has observed that reproductive choice is an insegragable part of a woman\\'s personal liberty as envisaged under Article 21 of the Constitution of India.\\n\"She cannot be forced to give birth to a child...She has a choice to give birth to the child or not,\" it observed.\\nThe petitioner was a minor who had committed murder and was in custody at an Observation Home. It was discovered by the Investigating Officer that she was pregnant because of sexual abuse. A crime under Protection of Children from Sexual Offences Act, 2012 was registered.\\n The petitioner pleaded that she is from an economically weak background, and has also undergone trauma due to sexual abuse, which she continues to suffer from. She pleaded that given her circumstances it would be difficult for her to raise a child. Neither was she equipped financially nor mentally. Moreover, this was an unwanted pregnancy.\\n The bench called for a Medical Report, which stated that her pregnancy was 16 weeks, yet it consented for termination of her pregnancy. The bench noted that Section 3 of the Medical Termination of Pregnancy Act, 1971 provides for termination of pregnancy:\\n when the length of the pregnancy does not exceed twelve weeksWhen the length of the pregnancy exceeds twelve weeks, if more than two registered medical practitioners are of the opinion that the termination is in good faith If the continuance of the pregnancy would involve a grave risk to the life or mental health of the pregnant woman If there is a substantial risk that if the child were born, it would suffer from serious physical or mental abnormalities \\n Explanation 1 further allows termination if the pregnancy is caused by rape as the anguish caused by such pregnancy shall constitute a grave injury to the mental health of the pregnant woman.\\n The High Court held that termination of pregnancy can be permitted post twelve weeks in certain situations. The bench noted that in the present case the petitioner is a minor and is unmarried. She is a victim of sexual abuse. Moreover, she is lodged in an Observational Home for an offence of murder. She is from an impoverished background. She also contends that the pregnancy is unwanted, and she is suffering from severe trauma.\\n The bench stated that declining permission to terminate the pregnancy would be tantamount to compelling her to continue with her pregnancy which in the circumstances will not only be a burden on her, but it would also cause grave injury to her mental health. Considering these observations, the court allowed the termination of the petitioner\\'s pregnancy regardless of her having completed sixteen weeks.'] ['इलाहाबाद हाईकोर्ट (Allahabad High Court) ने हाल ही में देखा कि उत्तर प्रदेश गैंगस्टर्स और असामाजिक गतिविधि (रोकथाम) अधिनियम, 1986 के तहत एक पिछले मामले में एक आरोपी की संलिप्तता के आधार पर एफआईआर दर्ज की सकती है।\\nजस्टिस सूर्य प्रकाश केसरवानी और जस्टिस पीयूष अग्रवाल की खंडपीठ ने रितेश कुमार उर्फ रिक्की बनाम यूपी राज्य एंड अन्य मामले में हाईकोर्ट के पहले के फैसले पर भरोसा करते हुए यह टिप्पणी की।\\nरितेश कुमार मामले में हाईकोर्ट ने माना था कि यूपी गैंगस्टर्स अधिनियम के तहत एक व्यक्ति की संलिप्तता के आधार पर भी एफआईआर दर्ज करना वैध और अनुमेय है।\\nइलाहाबाद हाईकोर्ट के समक्ष वर्तमान मामले में, अनवर शहजाद ने गैंगस्टर और असामाजिक गतिविधि अधिनियम 1986 की धारा 3 (1) के तहत उसके खिलाफ दर्ज प्राथमिकी को रद्द करने की मांग करते हुए अदालत का रुख किया था।\\nउसके द्वारा यह तर्क दिया गया था कि प्राथमिकी में, उनके खिलाफ आईपीसी की धारा 419, 420, 467, 468, 471, 120-बी और शस्त्र अधिनियम की धारा 30 के तहत मामला दर्ज है।  उसने आगे तर्क दिया कि वह निर्दोष है और उसे झूठा फंसाया गया है क्योंकि वह मुख्तार अंसारी का साला है।\\nयह आगे कहा गया कि वर्तमान सरकार ने सत्ताधारी पार्टी के उम्मीदवार के खिलाफ विधायक / संसदीय चुनाव लड़ने और जीतने वाले राजनीतिक प्रतिद्वंद्वी को परेशान करने की नीति शुरू की है।\\nदूसरी ओर, एजीए ने प्रस्तुत किया कि याचिकाकर्ता गैंग का सदस्य है, और अधिनियम, 1986 की धारा 2 (बी) के तहत अपराध करने की आदत है।\\nइसलिए, यह तर्क दिया गया कि पुलिस के पास उपलब्ध सामग्री के आधार पर आक्षेपित प्रथम सूचना रिपोर्ट का पंजीकरण गैंगस्टरों की सामग्री और दंडात्मक खंड यानी अधिनियम, 1986 की धारा 3 (1) को संतुष्ट करता है और इस प्रकार, प्रथम सूचना रिपोर्ट का पंजीकरण पूरी तरह से न्यायोचित ठहराता है।\\nकोर्ट ने इस बात पर जोर देते हुए कि प्रारंभिक चरण में आपराधिक कार्यवाही को बाधित नहीं किया जाना चाहिए और शिकायत / प्राथमिकी को रद्द करना एक सामान्य नियम के बजाय एक अपवाद होना चाहिए।\\nकोर्ट ने इस प्रकार राय दी,\\n\"निरस्त करने की शक्ति का प्रयोग दुर्लभतम मामलों में सावधानी के साथ किया जाना चाहिए। प्राथमिकी/शिकायत की जांच करते समय, जिसे रद्द करने की मांग की जाती है, न्यायालय आरोपों की विश्वसनीयता या वास्तविकता या अन्यथा के रूप में जांच शुरू नहीं कर सकता है।  प्राथमिकी/शिकायत में आमतौर पर, न्यायालयों को पुलिस के अधिकार क्षेत्र को हड़पने से रोक दिया जाता है, क्योंकि राज्य के दो अंग गतिविधियों के दो विशिष्ट क्षेत्रों में काम करते हैं और एक को दूसरे क्षेत्र में नहीं चलना चाहिए।  सूचना रिपोर्ट एक विश्वकोश नहीं है जिसमें रिपोर्ट किए गए अपराध के बारे में सभी तथ्यों और विवरणों का खुलासा होना चाहिए। इसलिए, जब पुलिस द्वारा जांच की जा रही है, तो अदालत को प्राथमिकी में लगाए गए आरोपों के मैरिट में नहीं जाना चाहिए। पुलिस को जांच पूरी करने की अनुमति दी जानी चाहिए।\"\\nइसके साथ ही कोर्ट ने याचिका खारिज कर दी।\\nसंबंधित समाचारों में, पिछले महीने सुप्रीम कोर्ट ने यह भी देखा कि उत्तर प्रदेश गैंगस्टर्स और असामाजिक गतिविधि (रोकथाम) अधिनियम, 1986 की धारा 2(बी) में उल्लिखित किसी भी असामाजिक गतिविधियों के लिए एक भी अपराध / प्राथमिकी / आरोप के मामले में एक व्यक्ति के खिलाफ मुकदमा चलाया जा सकता है।', 'बॉम्बे हाईकोर्ट (Bombay High Court) ने हाल ही में एक नाबालिग की 16 सप्ताह की गर्भ को समाप्त करने की अनुमति दी, जो यौन शोषण का शिकार थी और भारतीय दंड संहिता (IPC) की धारा 302 के तहत हत्या के अपराध के लिए एक ऑब्जर्वेशन होम में कस्टडी में है।\\nजस्टिस ए.एस. चंदुरकर और जस्टिस उर्मिला जोशी-फाल्के ने कहा कि सुप्रीम कोर्ट ने देखा है कि बच्चे को जन्म देने या न देना एक महिला की व्यक्तिगत स्वतंत्रता का एक अविभाज्य हिस्सा है जैसा कि भारत के संविधान के अनुच्छेद 21 के तहत प्रदान किया गया है।\\nकोर्ट ने कहा,\\n\"उसे बच्चे को जन्म देने के लिए मजबूर नहीं किया जा सकता। उसके पास बच्चे को जन्म देने या न करने का विकल्प है।\"\\nयाचिकाकर्ता नाबालिग है उसने हत्या का अपराध किया है और वह एक ऑब्जर्वेशन होम में हिरासत में है।  जांच अधिकारी को पता चला कि वह यौन शोषण के कारण गर्भवती है।  यौन अपराधों से बच्चों का संरक्षण अधिनियम, 2012 के तहत अपराध दर्ज किया गया था।\\nयाचिकाकर्ता ने दलील दी कि वह आर्थिक रूप से कमजोर पृष्ठभूमि से है, और यौन शोषण के कारण भी उसे आघात लगा है, जिसका वह लगातार सामना कर रही है।\\nउसने दलील दी कि उसकी परिस्थितियों को देखते हुए उसके लिए बच्चा पैदा करना मुश्किल होगा।  न तो वह आर्थिक रूप से सक्षम है और न ही मानसिक रूप से।  इसके अलावा, यह एक अवांछित गर्भ है।\\nपीठ ने एक मेडिकल रिपोर्ट मांगी, जिसमें देखा गया कि उसकी गर्भ 16 सप्ताह की है। हालांकि कोर्ट ने उसकी गर्भ को समाप्त करने के लिए सहमति दी।  पीठ ने कहा कि मेडिकल टर्मिनेशन ऑफ प्रेग्नेंसी एक्ट, 1971 की धारा 3 में गर्भ को समाप्त करने का प्रावधान है:\\n- जब गर्भ की अवधि बारह सप्ताह से अधिक न हो।\\n-  जब गर्भ की अवधि बारह सप्ताह से अधिक हो जाती है, यदि दो से अधिक पंजीकृत चिकित्सा चिकित्सकों की राय है कि समाप्ति सद्भाव में है।\\n- यदि गर्भ को जारी रखने से गर्भवती महिला के जीवन या मानसिक स्वास्थ्य को गंभीर खतरा होता है।\\n- यदि इस बात का पर्याप्त जोखिम है कि यदि बच्चा पैदा होता है, तो वह गंभीर शारीरिक या मानसिक असामान्यताओं से पीड़ित होगा।\\nस्पष्टीकरण 1 आगे गर्भ की अनुमति देता है यदि गर्भ बलात्कार के कारण होती है क्योंकि ऐसी गर्भ के कारण होने वाली पीड़ा गर्भवती महिला के मानसिक स्वास्थ्य के लिए गंभीर चोट होगी।\\nकोर्ट ने कहा कि कुछ स्थितियों में बारह सप्ताह के बाद गर्भ को समाप्त करने की अनुमति दी जा सकती है।\\nपीठ ने कहा कि वर्तमान मामले में याचिकाकर्ता नाबालिग है और अविवाहित है।  वह यौन शोषण की शिकार है।  इसके अलावा, वह हत्या के एक अपराध के लिए एक ऑब्जर्वेशन होम में रखा गया है।  वह एक गरीब पृष्ठभूमि से है।  वह यह भी तर्क देती है कि गर्भ अवांछित है, और वह गंभीर आघात से पीड़ित है।\\nपीठ ने कहा कि गर्भ को समाप्त करने की अनुमति देने से इनकार करना उसे गर्भ जारी रखने के लिए मजबूर करने के समान होगा जो न केवल उस पर बोझ होगा, बल्कि इससे उसके मानसिक स्वास्थ्य को भी गंभीर चोट पहुंचेगी।  इन टिप्पणियों को ध्यान में रखते हुए अदालत ने याचिकाकर्ता की गर्भ को समाप्त करने की अनुमति दी।']\n",
            "Validation Data Samples: ['The Allahabad High Court recently observed that a first information report under the Uttar Pradesh Gangsters and Anti-Social Activities (Prevention) Act, 1986 can be lodged on the basis of the involvement of an accused in a single previous case\\nThe Bench of Justice Surya Prakash Kesarwani and Justice Piyush Agrawal observed thus while relying upon an earlier judgment of the High Court in the case of Ritesh Kumar Alias Rikki vs. State of U.P. and another.\\nIn the Ritesh Kumar case, the HC had held that the lodging of a first information report under the UP Gangsters Act even on the basis of the involvement of a person in a single case, is valid and permissible.\\nIn the instant case before the Allahabad HC, one Anwar Shahzad had moved the Court seeking to quash an FIR registered against him under section 3 (1) of the Gangster & Anti Social Activities Act 1986.\\nIt was contended by him that in the impugned FIR, there is a mention of a Case against him under Sections 419, 420, 467, 468, 471, 120-B I.P.C., and Section 30 Arms Act. He further argued that he is innocent and has been falsely implicated because he is the real brother-in-law of Mukhtar Ansari.\\nIt was further stated that the present government has started a policy to harass the political opponent who had fought and won MLA/ parliamentary elections against the ruling party candidate.\\nOn the other hand, the AGA submitted that the petitioner is a member of the gang, and habitual of committing crimes as provided under Section 2(b) of the Act, 1986. \\nTherefore, it was argued, that the registration of the impugned First Information Report on the basis of materials available with the police satisfies the ingredients of gangsters and the penal clause i.e. Section 3 (1) of the Act, 1986, and thus, fully justifies registration of the impugned First Information Report.\\nAgainst this backdrop, the Court, while stressing that the criminal proceedings ought not to be scuttled at the initial stage and that quashing of complaint/FIR should be an exception rather than an ordinary rule, the Court opined thus:\\n\"The power of quashing should be exercised sparingly with circumspection, in the rarest of rare cases. While examining an FIR/complaint, quashing of which is sought, the Court cannot embark upon an enquiry as to the reliability or genuineness or otherwise of the allegations made in the FIR/complaint...Ordinarily, the Courts are barred from usurping the jurisdiction of the police, since the two organs of the State operate in two specific spheres of activities and one ought not to tread over the other sphere. The First Information Report is not an encyclopedia that must disclose all facts and details regarding the offence reported. Therefore, when the investigation by the police is in progress, the Court should not go into merits of the allegations made in the FIR. Police must be permitted to complete the investigation.\"\\nIn view of this observation, the Court dismissed the plea.\\nIn related news, last month the Supreme Court also observed that there can be a prosecution against a person under the Uttar Pradesh Gangsters and Anti-Social Activities (Prevention) Act, 1986, even in the case of a single offence/FIR/charge-sheet for any of the anti-social activities mentioned in section 2(b) of the Act.', 'The Bombay High Court recently allowed the termination of a 16 weeks\\' pregnancy of a minor who was a victim of sexual abuse and was also in custody at an Observation Home for a crime of murder under section 302 of the Indian Penal Code (IPC).\\n A bench of Justices A.S. Chandurkar and Urmila Joshi-Phalke noted that the Apex Court has observed that reproductive choice is an insegragable part of a woman\\'s personal liberty as envisaged under Article 21 of the Constitution of India.\\n\"She cannot be forced to give birth to a child...She has a choice to give birth to the child or not,\" it observed.\\nThe petitioner was a minor who had committed murder and was in custody at an Observation Home. It was discovered by the Investigating Officer that she was pregnant because of sexual abuse. A crime under Protection of Children from Sexual Offences Act, 2012 was registered.\\n The petitioner pleaded that she is from an economically weak background, and has also undergone trauma due to sexual abuse, which she continues to suffer from. She pleaded that given her circumstances it would be difficult for her to raise a child. Neither was she equipped financially nor mentally. Moreover, this was an unwanted pregnancy.\\n The bench called for a Medical Report, which stated that her pregnancy was 16 weeks, yet it consented for termination of her pregnancy. The bench noted that Section 3 of the Medical Termination of Pregnancy Act, 1971 provides for termination of pregnancy:\\n when the length of the pregnancy does not exceed twelve weeksWhen the length of the pregnancy exceeds twelve weeks, if more than two registered medical practitioners are of the opinion that the termination is in good faith If the continuance of the pregnancy would involve a grave risk to the life or mental health of the pregnant woman If there is a substantial risk that if the child were born, it would suffer from serious physical or mental abnormalities \\n Explanation 1 further allows termination if the pregnancy is caused by rape as the anguish caused by such pregnancy shall constitute a grave injury to the mental health of the pregnant woman.\\n The High Court held that termination of pregnancy can be permitted post twelve weeks in certain situations. The bench noted that in the present case the petitioner is a minor and is unmarried. She is a victim of sexual abuse. Moreover, she is lodged in an Observational Home for an offence of murder. She is from an impoverished background. She also contends that the pregnancy is unwanted, and she is suffering from severe trauma.\\n The bench stated that declining permission to terminate the pregnancy would be tantamount to compelling her to continue with her pregnancy which in the circumstances will not only be a burden on her, but it would also cause grave injury to her mental health. Considering these observations, the court allowed the termination of the petitioner\\'s pregnancy regardless of her having completed sixteen weeks.'] ['इलाहाबाद हाईकोर्ट (Allahabad High Court) ने हाल ही में देखा कि उत्तर प्रदेश गैंगस्टर्स और असामाजिक गतिविधि (रोकथाम) अधिनियम, 1986 के तहत एक पिछले मामले में एक आरोपी की संलिप्तता के आधार पर एफआईआर दर्ज की सकती है।\\nजस्टिस सूर्य प्रकाश केसरवानी और जस्टिस पीयूष अग्रवाल की खंडपीठ ने रितेश कुमार उर्फ रिक्की बनाम यूपी राज्य एंड अन्य मामले में हाईकोर्ट के पहले के फैसले पर भरोसा करते हुए यह टिप्पणी की।\\nरितेश कुमार मामले में हाईकोर्ट ने माना था कि यूपी गैंगस्टर्स अधिनियम के तहत एक व्यक्ति की संलिप्तता के आधार पर भी एफआईआर दर्ज करना वैध और अनुमेय है।\\nइलाहाबाद हाईकोर्ट के समक्ष वर्तमान मामले में, अनवर शहजाद ने गैंगस्टर और असामाजिक गतिविधि अधिनियम 1986 की धारा 3 (1) के तहत उसके खिलाफ दर्ज प्राथमिकी को रद्द करने की मांग करते हुए अदालत का रुख किया था।\\nउसके द्वारा यह तर्क दिया गया था कि प्राथमिकी में, उनके खिलाफ आईपीसी की धारा 419, 420, 467, 468, 471, 120-बी और शस्त्र अधिनियम की धारा 30 के तहत मामला दर्ज है।  उसने आगे तर्क दिया कि वह निर्दोष है और उसे झूठा फंसाया गया है क्योंकि वह मुख्तार अंसारी का साला है।\\nयह आगे कहा गया कि वर्तमान सरकार ने सत्ताधारी पार्टी के उम्मीदवार के खिलाफ विधायक / संसदीय चुनाव लड़ने और जीतने वाले राजनीतिक प्रतिद्वंद्वी को परेशान करने की नीति शुरू की है।\\nदूसरी ओर, एजीए ने प्रस्तुत किया कि याचिकाकर्ता गैंग का सदस्य है, और अधिनियम, 1986 की धारा 2 (बी) के तहत अपराध करने की आदत है।\\nइसलिए, यह तर्क दिया गया कि पुलिस के पास उपलब्ध सामग्री के आधार पर आक्षेपित प्रथम सूचना रिपोर्ट का पंजीकरण गैंगस्टरों की सामग्री और दंडात्मक खंड यानी अधिनियम, 1986 की धारा 3 (1) को संतुष्ट करता है और इस प्रकार, प्रथम सूचना रिपोर्ट का पंजीकरण पूरी तरह से न्यायोचित ठहराता है।\\nकोर्ट ने इस बात पर जोर देते हुए कि प्रारंभिक चरण में आपराधिक कार्यवाही को बाधित नहीं किया जाना चाहिए और शिकायत / प्राथमिकी को रद्द करना एक सामान्य नियम के बजाय एक अपवाद होना चाहिए।\\nकोर्ट ने इस प्रकार राय दी,\\n\"निरस्त करने की शक्ति का प्रयोग दुर्लभतम मामलों में सावधानी के साथ किया जाना चाहिए। प्राथमिकी/शिकायत की जांच करते समय, जिसे रद्द करने की मांग की जाती है, न्यायालय आरोपों की विश्वसनीयता या वास्तविकता या अन्यथा के रूप में जांच शुरू नहीं कर सकता है।  प्राथमिकी/शिकायत में आमतौर पर, न्यायालयों को पुलिस के अधिकार क्षेत्र को हड़पने से रोक दिया जाता है, क्योंकि राज्य के दो अंग गतिविधियों के दो विशिष्ट क्षेत्रों में काम करते हैं और एक को दूसरे क्षेत्र में नहीं चलना चाहिए।  सूचना रिपोर्ट एक विश्वकोश नहीं है जिसमें रिपोर्ट किए गए अपराध के बारे में सभी तथ्यों और विवरणों का खुलासा होना चाहिए। इसलिए, जब पुलिस द्वारा जांच की जा रही है, तो अदालत को प्राथमिकी में लगाए गए आरोपों के मैरिट में नहीं जाना चाहिए। पुलिस को जांच पूरी करने की अनुमति दी जानी चाहिए।\"\\nइसके साथ ही कोर्ट ने याचिका खारिज कर दी।\\nसंबंधित समाचारों में, पिछले महीने सुप्रीम कोर्ट ने यह भी देखा कि उत्तर प्रदेश गैंगस्टर्स और असामाजिक गतिविधि (रोकथाम) अधिनियम, 1986 की धारा 2(बी) में उल्लिखित किसी भी असामाजिक गतिविधियों के लिए एक भी अपराध / प्राथमिकी / आरोप के मामले में एक व्यक्ति के खिलाफ मुकदमा चलाया जा सकता है।', 'बॉम्बे हाईकोर्ट (Bombay High Court) ने हाल ही में एक नाबालिग की 16 सप्ताह की गर्भ को समाप्त करने की अनुमति दी, जो यौन शोषण का शिकार थी और भारतीय दंड संहिता (IPC) की धारा 302 के तहत हत्या के अपराध के लिए एक ऑब्जर्वेशन होम में कस्टडी में है।\\nजस्टिस ए.एस. चंदुरकर और जस्टिस उर्मिला जोशी-फाल्के ने कहा कि सुप्रीम कोर्ट ने देखा है कि बच्चे को जन्म देने या न देना एक महिला की व्यक्तिगत स्वतंत्रता का एक अविभाज्य हिस्सा है जैसा कि भारत के संविधान के अनुच्छेद 21 के तहत प्रदान किया गया है।\\nकोर्ट ने कहा,\\n\"उसे बच्चे को जन्म देने के लिए मजबूर नहीं किया जा सकता। उसके पास बच्चे को जन्म देने या न करने का विकल्प है।\"\\nयाचिकाकर्ता नाबालिग है उसने हत्या का अपराध किया है और वह एक ऑब्जर्वेशन होम में हिरासत में है।  जांच अधिकारी को पता चला कि वह यौन शोषण के कारण गर्भवती है।  यौन अपराधों से बच्चों का संरक्षण अधिनियम, 2012 के तहत अपराध दर्ज किया गया था।\\nयाचिकाकर्ता ने दलील दी कि वह आर्थिक रूप से कमजोर पृष्ठभूमि से है, और यौन शोषण के कारण भी उसे आघात लगा है, जिसका वह लगातार सामना कर रही है।\\nउसने दलील दी कि उसकी परिस्थितियों को देखते हुए उसके लिए बच्चा पैदा करना मुश्किल होगा।  न तो वह आर्थिक रूप से सक्षम है और न ही मानसिक रूप से।  इसके अलावा, यह एक अवांछित गर्भ है।\\nपीठ ने एक मेडिकल रिपोर्ट मांगी, जिसमें देखा गया कि उसकी गर्भ 16 सप्ताह की है। हालांकि कोर्ट ने उसकी गर्भ को समाप्त करने के लिए सहमति दी।  पीठ ने कहा कि मेडिकल टर्मिनेशन ऑफ प्रेग्नेंसी एक्ट, 1971 की धारा 3 में गर्भ को समाप्त करने का प्रावधान है:\\n- जब गर्भ की अवधि बारह सप्ताह से अधिक न हो।\\n-  जब गर्भ की अवधि बारह सप्ताह से अधिक हो जाती है, यदि दो से अधिक पंजीकृत चिकित्सा चिकित्सकों की राय है कि समाप्ति सद्भाव में है।\\n- यदि गर्भ को जारी रखने से गर्भवती महिला के जीवन या मानसिक स्वास्थ्य को गंभीर खतरा होता है।\\n- यदि इस बात का पर्याप्त जोखिम है कि यदि बच्चा पैदा होता है, तो वह गंभीर शारीरिक या मानसिक असामान्यताओं से पीड़ित होगा।\\nस्पष्टीकरण 1 आगे गर्भ की अनुमति देता है यदि गर्भ बलात्कार के कारण होती है क्योंकि ऐसी गर्भ के कारण होने वाली पीड़ा गर्भवती महिला के मानसिक स्वास्थ्य के लिए गंभीर चोट होगी।\\nकोर्ट ने कहा कि कुछ स्थितियों में बारह सप्ताह के बाद गर्भ को समाप्त करने की अनुमति दी जा सकती है।\\nपीठ ने कहा कि वर्तमान मामले में याचिकाकर्ता नाबालिग है और अविवाहित है।  वह यौन शोषण की शिकार है।  इसके अलावा, वह हत्या के एक अपराध के लिए एक ऑब्जर्वेशन होम में रखा गया है।  वह एक गरीब पृष्ठभूमि से है।  वह यह भी तर्क देती है कि गर्भ अवांछित है, और वह गंभीर आघात से पीड़ित है।\\nपीठ ने कहा कि गर्भ को समाप्त करने की अनुमति देने से इनकार करना उसे गर्भ जारी रखने के लिए मजबूर करने के समान होगा जो न केवल उस पर बोझ होगा, बल्कि इससे उसके मानसिक स्वास्थ्य को भी गंभीर चोट पहुंचेगी।  इन टिप्पणियों को ध्यान में रखते हुए अदालत ने याचिकाकर्ता की गर्भ को समाप्त करने की अनुमति दी।']\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Function to load pickle data\n",
        "def load_pickle_data(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Load train data\n",
        "train_eng_summary, train_hindi_summary = load_pickle_data(\"/content/drive/MyDrive/pick/train_data.pkl\")\n",
        "\n",
        "# Load test data\n",
        "test_eng_summary, test_hindi_summary = load_pickle_data(\"/content/drive/MyDrive/pick/test_data.pkl\")\n",
        "\n",
        "# Load validation data\n",
        "val_eng_summary, val_hindi_summary = load_pickle_data(\"/content/drive/MyDrive/pick/val_data.pkl\")\n",
        "\n",
        "# Print some sample data to verify\n",
        "print(\"Train Data Samples:\", train_eng_summary[:2], train_hindi_summary[:2])\n",
        "print(\"Test Data Samples:\", test_eng_summary[:2], test_hindi_summary[:2])\n",
        "print(\"Validation Data Samples:\", val_eng_summary[:2], val_hindi_summary[:2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_summary[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4x9B6NnmEYF8",
        "outputId": "a70b6d8d-bb92-4d05-a7ac-fd9f5523fd43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Delhi High Court has observed that a momentary lapse on the part of the candidate must not be met with such a severe punitive action which would cause grave and irreparable prejudice and affect the candidate\\'s future perversely.\\nJustice Sanjeev Narula granted relief to a candidate namely Samriddhi Khandelwal by directing National Institute of Fashion Technology (NIFT) to allow her to join the counselling on the basis of her results of the online invigilated or remote proctored NIFT Entrance Exam, 2022. \\nThe said Exam was divided into two parts   a written exam which was held on 06th February and a Situation Test which was to be held from 2nd April 2022 onwards. \\nThe petitioner candidate had appeared for the online exam on 06th February 2022 from her residence, as per specifications mentioned in the Admit Card, complying with Rules and Regulations for Online Invigilated / Remote Proctored NIFT Entrance Exam for Admissions 2022. \\nShe was then declared as \"disqualified\" in respect of the CAT exam, in the declaration of result dated 9th March, 2022. Aggrieved by the same, the petition was filed seeking directions on NIFT to publish and declare her in pursuance of the entrance exam for admission to bachelor of design course. The plea also sought directions on NIFT to allow her to participate in Situation Test (second round).\\nWhile the Court allowed the plea, it was of the view that it cannot enter the mind of the Petitioner to establish her bona fides, however, it was satisfied that she had come with clean hands and had repeatedly admitted to the lapse on her part while clicking photos and uploading the PDFs in the said exam. \\n\"The Court is also mindful of the fact that due to the advent of COVID-19 pandemic, the structure of examinations has certainly undergone a monumental change, rendering both students and educational institutions alike, struggling to cope-up with the challenges posed by a complete shift to the virtual world. The consequences of such an upheaval have deep implications, and the instant petition is just one of the many issues that have been flagged as a by-product of this shift,\" the Court observed. \\nIt was the case of NIFT that the Petitioner was rightly disqualified as she had disclosed the identity of the institute from which she was taking coaching for the exam, which was clearly visible from the PDF answer sheets uploaded by her to the online portal. Therefore, NIFT had argued that it was a case of her identity being disclosed in the examination, which was in violation of Rule 12 printed on the Admit Card. \\nIn this backdrop, the Court was of the view:\\n\"Though the afore-noted pictures reveal the name of the coaching institute, however, they do not indicate the identity of the candidate. This identification mark does not seem to violate condition no. 12 of the Admit Card, and for this reason alone, the Court is inclined to give the benefit of the doubt to the Petitioner. Surely, the identification of a candidate can also be revealed by giving a hint or a suggestion, but that can occur if there is a nexus between the candidate and the examiner. However, such a case is not canvassed by the Respondent.\"\\nThe Court also noted that there was no allegation against the Petitioner candidate of any malpractice or use of unfair means and that she was required to analyze the question and draw her answers on the questions-cum-answer sheets. \\nThe Court also noted that the petitioner was then required to click pictures thereof, stitch them together into PDF files, and upload them on the portal, all within the stipulated time. \\n\"It cannot be forgotten that the Petitioner is just a teenager, and given the enormity of appearing for an entrance exam which decides her career, coupled with the obligations of a new-format online system, it could have taken a toll on the Petitioner who was in a pinch of time. It is therefore plausible that due to paucity of time, towards the fag end of the exam, the Petitioner panicked and did not crop or properly click photographs of her answer sheets and hurriedly uploaded the same to conform with the deadline,\" it said. \\nThe Court added \"In view of the foregoing, the Court is of the view that a momentary lapse on the part of the candidate must not be met with such a severe punitive action, which would cause grave and irreparable prejudice and affect the Petitioner\\'s future perversely.\"\\nThe plea was accordingly allowed. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLT1UaCKKGhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Performer mt5"
      ],
      "metadata": {
        "id": "NbegXhgoLzzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MT5Config, MT5Tokenizer, PreTrainedModel\n",
        "from transformers.models.mt5.modeling_mt5 import MT5Stack, MT5ForConditionalGeneration\n",
        "from performer_pytorch import SelfAttention\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Custom Performer-based Attention wrapper for integration\n",
        "class PerformerSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, config, is_decoder=False):\n",
        "        super().__init__()\n",
        "        self.performer = SelfAttention(\n",
        "            dim=config.d_model,\n",
        "            heads=config.num_heads,\n",
        "            nb_features=128,  # Reduced for memory efficiency\n",
        "            causal=is_decoder\n",
        "        )\n",
        "        self.is_decoder = is_decoder\n",
        "        self.dropout = torch.nn.Dropout(config.dropout_rate)\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        position_bias=None,\n",
        "        past_key_value=None,\n",
        "        layer_head_mask=None,\n",
        "        output_attentions=False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # Apply Performer attention (without attention weights)\n",
        "        hidden_states = self.performer(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        # Required outputs in MT5 format:\n",
        "        # (hidden_states, present_key_value, position_bias, attention_weights, cross_attn_weights)\n",
        "        return (\n",
        "            hidden_states,     # hidden states after attention\n",
        "            None,              # present_key_value (for cache)\n",
        "            None,              # position_bias (used for relative positions in T5)\n",
        "            None,              # attention_weights (Performer doesn't output them)\n",
        "            None               # cross-attn weights (only relevant in decoder cross-attn)\n",
        "        )\n",
        "\n",
        "# Custom MT5 Encoder/Decoder block with Performer\n",
        "class PerformerMT5Model(MT5ForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        # Replace encoder self-attention with Performer\n",
        "        for block in self.encoder.block:\n",
        "            block.layer[0].SelfAttention = PerformerSelfAttention(config, is_decoder=False)\n",
        "\n",
        "        # Replace decoder self-attention and cross-attention with Performer\n",
        "        for block in self.decoder.block:\n",
        "            block.layer[0].SelfAttention = PerformerSelfAttention(config, is_decoder=True)\n",
        "            block.layer[1].EncDecAttention = PerformerSelfAttention(config, is_decoder=False)\n",
        "\n",
        "        self.model = MT5Stack(config, self.shared)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return super().forward(**kwargs)"
      ],
      "metadata": {
        "id": "tobabrdTL8Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5Tokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"google/mt5-base\"\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name, legacy=False)\n",
        "config = MT5Config.from_pretrained(model_name)\n",
        "tokenizer.model_max_length = 2048\n",
        "\n",
        "# Update config for long input\n",
        "config.max_length = 2048\n",
        "config.attention_probs_dropout_prob = 0.1\n",
        "\n",
        "# Initialize custom Performer MT5 model\n",
        "model = PerformerMT5Model(config).to(device)\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsVlDIAaMG0U",
        "outputId": "3e395af4-171f-4bf1-9892-8b72976b5b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(eng_summary, hindi_summary, tokenizer):\n",
        "    input_encodings = tokenizer(\n",
        "        eng_summary,\n",
        "        truncation=True,\n",
        "        padding=\"longest\",  # dynamically pads to the longest in the batch\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    target_encodings = tokenizer(\n",
        "        hindi_summary,\n",
        "        truncation=True,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_encodings['input_ids'].squeeze(),\n",
        "        'attention_mask': input_encodings['attention_mask'].squeeze(),\n",
        "        'labels': target_encodings['input_ids'].squeeze()\n",
        "    }"
      ],
      "metadata": {
        "id": "VU-ev1nkMRmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, eng_summaries, hindi_summaries, tokenizer):\n",
        "        self.eng_summaries = eng_summaries\n",
        "        self.hindi_summaries = hindi_summaries\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng_summaries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return preprocess_data(\n",
        "            self.eng_summaries[idx],\n",
        "            self.hindi_summaries[idx],\n",
        "            self.tokenizer\n",
        "        )"
      ],
      "metadata": {
        "id": "phHqv_SyNSRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "train_dataset = TranslationDataset(train_eng_summary, train_hindi_summary, tokenizer)\n",
        "val_dataset = TranslationDataset(val_eng_summary, val_hindi_summary, tokenizer)\n",
        "test_dataset = TranslationDataset(test_eng_summary, test_hindi_summary, tokenizer)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, collate_fn=data_collator)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "V-yQNS_1NXBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
      ],
      "metadata": {
        "id": "7xz4OnaqN9kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torch.amp import GradScaler\n",
        "\n",
        "#gradient_accumulation_steps = 4  # Simulates batch_size=4 while using batch_size=2\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs=100, patience=4):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model_path = '/content/drive/MyDrive/pick/best_long_model.pt'\n",
        "    scaler = GradScaler(enabled=True)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Mixed precision training\n",
        "            #with autocast(device_type='cuda'):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Training loss: {avg_train_loss}\")\n",
        "\n",
        "        val_loss = evaluate_model(model, val_dataloader)\n",
        "        print(f\"Validation loss: {val_loss}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved best model at epoch {epoch+1}\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Training complete. Best model saved at {best_model_path}\")\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "HqEAkAkxOT2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_loss / len(val_dataloader)\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "V2D88HzUOdQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # 3. Reset peak memory stats\n",
        "# torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# print(torch.cuda.memory_allocated())  # This shows the amount of memory currently allocated\n",
        "# print(torch.cuda.memory_reserved())   # This shows the amount of memory reserved by the CUDA memory allocator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3IDV2UrTPZy",
        "outputId": "ca5f1f46-e536-4a66-d0f3-2f559e30c1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8116401152\n",
            "8378122240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(model, train_dataloader, val_dataloader, optimizer, scheduler)\n",
        "\n",
        "# Optionally, evaluate the best model on the test set\n",
        "test_loss = evaluate_model(trained_model, test_dataloader)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VM8El7ebOffY",
        "outputId": "87b632a2-150c-403c-cb20-844dd94c8f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: the provided PTX was compiled with an unsupported toolchain.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0075fa4e3bf9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Optionally, evaluate the best model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-10207bbc84c7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Mixed precision training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#with autocast(device_type='cuda'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-81276d238bef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1930\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1931\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m   1101\u001b[0m                     \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     ):\n\u001b[0;32m--> 559\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    560\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mt5/modeling_mt5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    473\u001b[0m     ):\n\u001b[1;32m    474\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-81276d238bef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, past_key_value, layer_head_mask, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     ):\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Apply Performer attention (without attention weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/performer_pytorch/performer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'self attention should not receive context'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCrossAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/performer_pytorch/performer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, pos_emb, context, mask, context_mask, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mattn_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/performer_pytorch/performer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mattn_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_attention\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal_linear_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/performer_pytorch/performer_pytorch.py\u001b[0m in \u001b[0;36mcausal_linear_attention\u001b[0;34m(q, k, v, eps)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_dot_product_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...nd,...n->...nd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: the provided PTX was compiled with an unsupported toolchain.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mt5-base"
      ],
      "metadata": {
        "id": "N649qd1PG2G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "VG6HDBYIC5O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "\n",
        "# Device setup (prefer GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained M-T5 model and tokenizer\n",
        "model_name = 'google/mt5-base'\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Use MT5Tokenizer explicitly\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name, legacy=False)\n",
        "\n",
        "# Now you can use the tokenizer and model together"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2vVyZH_Ec3p",
        "outputId": "5197362e-8022-499a-c9e6-f8b3becc1492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function for tokenizing data\n",
        "def preprocess_data(eng_summary, hindi_summary, tokenizer, max_input_length=512, max_output_length=128):\n",
        "    input_encodings = tokenizer(eng_summary, truncation=True, padding='max_length', max_length=max_input_length, return_tensors=\"pt\")\n",
        "    target_encodings = tokenizer(hindi_summary, truncation=True, padding='max_length', max_length=max_output_length, return_tensors=\"pt\")\n",
        "\n",
        "    return {'input_ids': input_encodings['input_ids'].squeeze(),\n",
        "            'attention_mask': input_encodings['attention_mask'].squeeze(),\n",
        "            'labels': target_encodings['input_ids'].squeeze()}"
      ],
      "metadata": {
        "id": "v09FL6qkEfXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom dataset class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, eng_summaries, hindi_summaries, tokenizer, max_input_length=512, max_output_length=128):\n",
        "        self.eng_summaries = eng_summaries\n",
        "        self.hindi_summaries = hindi_summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_output_length = max_output_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng_summaries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return preprocess_data(self.eng_summaries[idx], self.hindi_summaries[idx], self.tokenizer, self.max_input_length, self.max_output_length)"
      ],
      "metadata": {
        "id": "3k60vcOVEimA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the datasets\n",
        "train_dataset = TranslationDataset(train_eng_summary, train_hindi_summary, tokenizer)\n",
        "val_dataset = TranslationDataset(val_eng_summary, val_hindi_summary, tokenizer)\n",
        "test_dataset = TranslationDataset(test_eng_summary, test_hindi_summary, tokenizer)\n",
        "\n",
        "# DataLoader setup\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "MVehWVJZElWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
      ],
      "metadata": {
        "id": "Z-QOaOBrEz93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with Early Stopping and ReduceLROnPlateau\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs=100, patience=4):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model_path = '/content/drive/MyDrive/pick/best_mt5_model.pt'\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Training phase\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_dataloader)\n",
        "        print(f\"Training loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss = evaluate_model(model, val_dataloader)\n",
        "        print(f\"Validation loss: {val_loss}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved best model at epoch {epoch+1}\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        # Early stopping if no improvement\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        # Adjust learning rate if necessary\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Training complete. Best model saved at {best_model_path}\")\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "9alieZLME2B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "def evaluate_model(model, val_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_loss / len(val_dataloader)\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "LkEtRti2GQN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process\n",
        "trained_model = train_model(model, train_dataloader, val_dataloader, optimizer, scheduler)\n",
        "\n",
        "# Optionally, evaluate the best model on the test set\n",
        "test_loss = evaluate_model(trained_model, test_dataloader)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gux5k2YGGWTF",
        "outputId": "969939ad-5255-49d3-ed63-b4e9a3c03a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.094889259502053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.0382729042799044\n",
            "Saved best model at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 1.6046608137047809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.3993469339150648\n",
            "Saved best model at epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 1.1302211129283468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.1499424379987595\n",
            "Saved best model at epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.8169876683835853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.0663208280108933\n",
            "Saved best model at epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.5865377715134511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.0942461234318395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.41553572275477635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.182604479086068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.2956916016865377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.2955959238844295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.1459285473779219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.5707359995979528\n",
            "Early stopping at epoch 8\n",
            "Training complete. Best model saved at /content/drive/MyDrive/pick/best_mt5_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0663208280108933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Define model name\n",
        "model_name = 'google/mt5-base'\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model architecture\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Load the trained weights\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/pick/best_mt5_model.pt\", map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
        "\n",
        "# Move model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWMgztYNJrzT",
        "outputId": "dffc29b6-49f5-4bef-c8f4-98c371475a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MT5ForConditionalGeneration(\n",
              "  (shared): Embedding(250112, 768)\n",
              "  (encoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_segments(text, tokenizer, max_tokens=400):\n",
        "    \"\"\"\n",
        "    Split text into segments of <= `max_tokens` (leaving room for special tokens).\n",
        "    Uses sentence boundaries for better translation coherence.\n",
        "    \"\"\"\n",
        "    sentences = text.split('. ')  # Simple split (improve with NLP libs like spaCy)\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence += \". \"  # Re-add delimiter\n",
        "        if len(tokenizer.encode(current_segment + sentence)) <= max_tokens:\n",
        "            current_segment += sentence\n",
        "        else:\n",
        "            if current_segment:\n",
        "                segments.append(current_segment.strip())\n",
        "            current_segment = sentence\n",
        "\n",
        "    if current_segment:\n",
        "        segments.append(current_segment.strip())\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "KXtYLS7Ilzie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_long_text(model, tokenizer, english_text, device='cuda', max_input_tokens=512):\n",
        "    # Split into manageable segments\n",
        "    segments = split_text_into_segments(english_text, tokenizer, max_tokens=max_input_tokens)\n",
        "    hindi_translations = []\n",
        "\n",
        "    for segment in segments:\n",
        "        inputs = tokenizer(\n",
        "            segment,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_input_tokens\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=150,\n",
        "            num_beams=4,\n",
        "            no_repeat_ngram_size=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        hindi_translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        hindi_translations.append(hindi_translation)\n",
        "\n",
        "    # Combine translations (add space between segments)\n",
        "    full_hindi_translation = \" \".join(hindi_translations)\n",
        "    return full_hindi_translation"
      ],
      "metadata": {
        "id": "15D6V8TJl0NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_translation = translate_long_text(model, tokenizer, test_eng_summary[1], device)\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_repetition(hindi_text):\n",
        "    # Fix repeated phrases (e.g., \"समाप्ति की समाप्ति...\")\n",
        "    cleaned_text = re.sub(r'(\\b\\w+\\b)(?:\\s+\\1)+', r'\\1', hindi_text)\n",
        "    return cleaned_text\n",
        "\n",
        "clean_translation = clean_repetition(hindi_translation)\n",
        "print(clean_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSmeLaW2l64q",
        "outputId": "d0327c9b-610f-4bd6-c7df-d1f1cf9c854e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "बॉम्बे हाईकोर्ट ने हाल ही में भारतीय दंड संहिता (आईपीसी) की धारा 302 के तहत बलात्कार का शिकार हुएक नाबालिग की 16 सप्ताह की गर्भपात की रोक लगाने की अनुमति दी। जस्टिस ए.एस. चंडूरकर और ज स्टिस यू.आर. जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने कहा है कि एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है, जैसा इलाहाबाद हाईकोर्ट ने कहा कि गर्भावस्था की समाप्ति की अनुमति दी जा सकती है, जब तक कि गर्भ की लंबाई दस सप्ताह नहीं हो जाती, तब तक गर्भवती महिला के जीवन या मानसिक स्वास्थ्य के लिएक गंभीर नुकसान हो। यदि गर्भ के लंबे समय के बीच प्रेगनेंसी की समय सीमा अधिक है, तो यह निर्धारित किया जा सकता है कि यह पूर्ण निष्पक्षता\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def translate_long_text_mt5(input_text, tokenizer, model):\n",
        "    sentences = sent_tokenize(input_text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        temp_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "        if len(tokenizer.encode(\"translate English to Hindi: \" + temp_chunk)) < 512:\n",
        "            current_chunk = temp_chunk\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    # Translate chunks\n",
        "    translations = []\n",
        "    for chunk in chunks:\n",
        "        prompt = \"translate English to Hindi: \" + chunk\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            max_length=512\n",
        "        )\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=256,  # safer than max_length\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            length_penalty=0.9,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        translations.append(translated_text.strip())\n",
        "\n",
        "    return \"\\n\".join(translations)\n",
        "\n",
        "\n",
        "hindi_summary = translate_long_text_mt5(test_eng_summary[1], tokenizer, model)\n",
        "print(\"English Summary:\", test_eng_summary[1])\n",
        "print(\"Hindi Summary:\", hindi_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMS5jpECQjd5",
        "outputId": "153b07e5-815e-4cc8-f0bf-4169670a24fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Summary: The Bombay High Court recently allowed the termination of a 16 weeks' pregnancy of a minor who was a victim of sexual abuse and was also in custody at an Observation Home for a crime of murder under section 302 of the Indian Penal Code (IPC).\n",
            " A bench of Justices A.S. Chandurkar and Urmila Joshi-Phalke noted that the Apex Court has observed that reproductive choice is an insegragable part of a woman's personal liberty as envisaged under Article 21 of the Constitution of India.\n",
            "\"She cannot be forced to give birth to a child...She has a choice to give birth to the child or not,\" it observed.\n",
            "The petitioner was a minor who had committed murder and was in custody at an Observation Home. It was discovered by the Investigating Officer that she was pregnant because of sexual abuse. A crime under Protection of Children from Sexual Offences Act, 2012 was registered.\n",
            " The petitioner pleaded that she is from an economically weak background, and has also undergone trauma due to sexual abuse, which she continues to suffer from. She pleaded that given her circumstances it would be difficult for her to raise a child. Neither was she equipped financially nor mentally. Moreover, this was an unwanted pregnancy.\n",
            " The bench called for a Medical Report, which stated that her pregnancy was 16 weeks, yet it consented for termination of her pregnancy. The bench noted that Section 3 of the Medical Termination of Pregnancy Act, 1971 provides for termination of pregnancy:\n",
            " when the length of the pregnancy does not exceed twelve weeksWhen the length of the pregnancy exceeds twelve weeks, if more than two registered medical practitioners are of the opinion that the termination is in good faith If the continuance of the pregnancy would involve a grave risk to the life or mental health of the pregnant woman If there is a substantial risk that if the child were born, it would suffer from serious physical or mental abnormalities \n",
            " Explanation 1 further allows termination if the pregnancy is caused by rape as the anguish caused by such pregnancy shall constitute a grave injury to the mental health of the pregnant woman.\n",
            " The High Court held that termination of pregnancy can be permitted post twelve weeks in certain situations. The bench noted that in the present case the petitioner is a minor and is unmarried. She is a victim of sexual abuse. Moreover, she is lodged in an Observational Home for an offence of murder. She is from an impoverished background. She also contends that the pregnancy is unwanted, and she is suffering from severe trauma.\n",
            " The bench stated that declining permission to terminate the pregnancy would be tantamount to compelling her to continue with her pregnancy which in the circumstances will not only be a burden on her, but it would also cause grave injury to her mental health. Considering these observations, the court allowed the termination of the petitioner's pregnancy regardless of her having completed sixteen weeks.\n",
            "Hindi Summary: बॉम्बे हाईकोर्ट ने हाल ही में भारतीय दंड संहिता (आईपीसी) की धारा 302 के तहत बलात्कार का शिकार हुए एक नाबालिग की 16 सप्ताह की गर्भपात को रद्द करने की अनुमति दी। जस्टिस एएस चंडूरकर और ज स्टिस अतुल जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट भारत के संविधान के अनुच्छेद 21 के अनुसार महिलाओं का व्यक्तिगत स्वतंत्रता एक अनिवार्य भाग है।\n",
            "इलाहाबाद हाईकोर्ट ने कहा कि गर्भावस्था की समाप्ति के लिए मेडिकल टर्मिनेशन ऑफ प्रेगनेंसी एक्ट, 1971 की धारा 3 यह प्रदान करती है: जब गर्भ की अवधि दस सप्ताह से अधिक नहीं होती, यदि गर्भवती होने की आयु कम है, तो उसकी लंबी लम्बे की दूरी तक नहीं बढ़ जाती थी। इसके विपरीत, अगर पति अपनी नाबालिग से शादी कर रही\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_summary(model, tokenizer, input_text, max_length=512):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    inputs = tokenizer(\"Translate to hindi: \"+input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    print(\"Input token length:\", len(inputs[\"input_ids\"][0]))  # Debug token length\n",
        "    # Generate output (Hindi summary)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, num_beams=7, no_repeat_ngram_size=2, length_penalty = 0.5, max_length=max_length)\n",
        "    print(\"Output token length:\", len(output_ids[0]))  # Debug token length\n",
        "    # Decode output tokens to Hindi text\n",
        "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "\n",
        "hindi_summary = generate_summary(model, tokenizer, test_eng_summary[2])\n",
        "\n",
        "print(\"English Summary:\", test_eng_summary[2])\n",
        "print(\"Hindi Summary:\", hindi_summary)"
      ],
      "metadata": {
        "id": "7RkwBZRVGXJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90b28a5-1c0a-440a-9954-85d10166a6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token length: 512\n",
            "Output token length: 131\n",
            "English Summary: The Bombay High Court recently allowed the termination of a 16 weeks' pregnancy of a minor who was a victim of sexual abuse and was also in custody at an Observation Home for a crime of murder under section 302 of the Indian Penal Code (IPC).\n",
            " A bench of Justices A.S. Chandurkar and Urmila Joshi-Phalke noted that the Apex Court has observed that reproductive choice is an insegragable part of a woman's personal liberty as envisaged under Article 21 of the Constitution of India.\n",
            "\"She cannot be forced to give birth to a child...She has a choice to give birth to the child or not,\" it observed.\n",
            "The petitioner was a minor who had committed murder and was in custody at an Observation Home. It was discovered by the Investigating Officer that she was pregnant because of sexual abuse. A crime under Protection of Children from Sexual Offences Act, 2012 was registered.\n",
            " The petitioner pleaded that she is from an economically weak background, and has also undergone trauma due to sexual abuse, which she continues to suffer from. She pleaded that given her circumstances it would be difficult for her to raise a child. Neither was she equipped financially nor mentally. Moreover, this was an unwanted pregnancy.\n",
            " The bench called for a Medical Report, which stated that her pregnancy was 16 weeks, yet it consented for termination of her pregnancy. The bench noted that Section 3 of the Medical Termination of Pregnancy Act, 1971 provides for termination of pregnancy:\n",
            " when the length of the pregnancy does not exceed twelve weeksWhen the length of the pregnancy exceeds twelve weeks, if more than two registered medical practitioners are of the opinion that the termination is in good faith If the continuance of the pregnancy would involve a grave risk to the life or mental health of the pregnant woman If there is a substantial risk that if the child were born, it would suffer from serious physical or mental abnormalities \n",
            " Explanation 1 further allows termination if the pregnancy is caused by rape as the anguish caused by such pregnancy shall constitute a grave injury to the mental health of the pregnant woman.\n",
            " The High Court held that termination of pregnancy can be permitted post twelve weeks in certain situations. The bench noted that in the present case the petitioner is a minor and is unmarried. She is a victim of sexual abuse. Moreover, she is lodged in an Observational Home for an offence of murder. She is from an impoverished background. She also contends that the pregnancy is unwanted, and she is suffering from severe trauma.\n",
            " The bench stated that declining permission to terminate the pregnancy would be tantamount to compelling her to continue with her pregnancy which in the circumstances will not only be a burden on her, but it would also cause grave injury to her mental health. Considering these observations, the court allowed the termination of the petitioner's pregnancy regardless of her having completed sixteen weeks.\n",
            "Hindi Summary: दिल्ली हाईकोर्ट (Delhi High Court) ने कहा कि किसी भी उम्मीदवार की भागीदारी पर थोड़ी देरी नहीं की जानी चाहिए, जिसके परिणाम उससे गंभीर और अवांछनीय रूप से प्रभावित होगा। जस्टिस संजीव नरूला ने नेशनल इंस्टीट्यूट ऑफ फैशन टेक्नोलॉजी (एनआईएफटी) को निर्देश दिया कि उसके पास नॉन-ऑनलाइन ऑफिशियल ऑडिशनल इंटर्नशिप, 2022 के रिज\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3jt98xdLFhj",
        "outputId": "c21b757f-b18a-4f72-a484-c84788cd095b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in sent_tokenize(test_eng_summary[2]):\n",
        "    print(i)\n",
        "\n",
        "    hindi_summary = generate_summary(model, tokenizer, i)\n",
        "    print(\"hindi: \", hindi_summary)\n",
        "\n",
        "    print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "# tokenizer.tokenize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MDOoTz9L6Cj",
        "outputId": "98ee3ae3-486b-4ff9-c3e7-ffb440f2746d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Delhi High Court has observed that a momentary lapse on the part of the candidate must not be met with such a severe punitive action which would cause grave and irreparable prejudice and affect the candidate's future perversely.\n",
            "Input token length: 58\n",
            "Output token length: 128\n",
            "hindi:  दिल्ली हाईकोर्ट ने माना है कि किसी उम्मीदवार पर तत्काल देरी का विरोध करने की आवश्यकता नहीं होनी चाहिए, जिससे भाजपा की भविष्य में गंभीर और अप्रत्याशित हो। दिल्ली उच्च न्यायालय ने कहा कि पार्टी की ओर से होने वाला समय-सीमा के साथ ही यह कठोर दंडात्मक कार्रवाई को उससे कम से कम नजरअंदाज से नहीं किया जा सकता है, जो उम्मीदवारों\n",
            "------------------------------------------------------------------------\n",
            "Justice Sanjeev Narula granted relief to a candidate namely Samriddhi Khandelwal by directing National Institute of Fashion Technology (NIFT) to allow her to join the counselling on the basis of her results of the online invigilated or remote proctored NIFT Entrance Exam, 2022.\n",
            "Input token length: 72\n",
            "Output token length: 129\n",
            "hindi:  जस्टिस संजीव नरूला ने नेशनल इंस्टीट्यूट ऑफ फैशन टेक्नोलॉजी (एनआईएफ) को यह राहत देने का निर्देश दिया, जिसमें समिदि दिव्यांशु कांस्टेबल को अनिवार्य करने की अनुमति दी गई थी। इसके साथ ही राष्ट्रीय फिजिकल तकनीकी विकास आयोग (न्यूड) के लिए उसके परिणामों के आधार पर निश्चित रूप से NIFT विश्वसनीयता परीक्षा, 2022\n",
            "------------------------------------------------------------------------\n",
            "The said Exam was divided into two parts   a written exam which was held on 06th February and a Situation Test which was to be held from 2nd April 2022 onwards.\n",
            "Input token length: 44\n",
            "Output token length: 123\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने 6 फरवरी, 2022 को होने वाली मेडिकल एक्जामिनेशन (Written-Test) के रूप में आयोजित किया जा रहा था। यह जांच एक लिखित एग्जाम के दो भागों में से एक परीक्षा जिसमें शीर्षक प्राप्त करने की अनुमति दी गई थी। इसके साथ ही उसकी स्थिति टेस्ट (Situation Test) की तारीख 2 अप्रैल 2022 तक होनी चाहिए।\n",
            "------------------------------------------------------------------------\n",
            "The petitioner candidate had appeared for the online exam on 06th February 2022 from her residence, as per specifications mentioned in the Admit Card, complying with Rules and Regulations for Online Invigilated / Remote Proctored NIFT Entrance Exam for Admissions 2022.\n",
            "Input token length: 69\n",
            "Output token length: 125\n",
            "hindi:  सुप्रीम कोर्ट ने 6 फरवरी 2022 को ऑनलाइन परीक्षा के लिए आयोजित होने वाले पात्रता मानदंडों के अनुपालन के अधीन किया गया है। याचिकाकर्ता ने आधिकारिक रूप से अपने निवास के आधार पर एनआईएफटी एडमिस्ट्रेटेड नेशनल इंटर्नशिप के दौरान उसका घर से आवेदन करने की अनुमति दी है, जिसके अनुसार उन्होंने उसी तारीख से उसके आवास से\n",
            "------------------------------------------------------------------------\n",
            "She was then declared as \"disqualified\" in respect of the CAT exam, in the declaration of result dated 9th March, 2022.\n",
            "Input token length: 33\n",
            "Output token length: 41\n",
            "hindi:  इलाहाबाद हाईकोर्ट (CAT) परीक्षा की तारीख 9 मार्च, 2022 को आयोजित किया गया था। यह टिप्पणियां\n",
            "------------------------------------------------------------------------\n",
            "Aggrieved by the same, the petition was filed seeking directions on NIFT to publish and declare her in pursuance of the entrance exam for admission to bachelor of design course.\n",
            "Input token length: 47\n",
            "Output token length: 125\n",
            "hindi:  सुप्रीम को ग्रेजुएट की डिग्री कोर्स में एडमिशन प्राप्त करने के लिए एनआईएफटी पर निर्देश देने की मांग की गई थी। इसके साथ ही, नेशनल इंटर्नशिप काउंसिल पर नियोक्ता की ओर से दायर याचिका का विरोध किया गया था। नई दिल्ली सरकार पर NIFT द्वारा बीमा पाठ्यक्रम की शुरुआत परीक्षा के दौरान और उसके संबंध में नोटिस जारी\n",
            "------------------------------------------------------------------------\n",
            "The plea also sought directions on NIFT to allow her to participate in Situation Test (second round).\n",
            "Input token length: 27\n",
            "Output token length: 31\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने एनआईएफटी पर NIFT पर रोक लगाने की मांग की गई थी।\n",
            "------------------------------------------------------------------------\n",
            "While the Court allowed the plea, it was of the view that it cannot enter the mind of the Petitioner to establish her bona fides, however, it was satisfied that she had come with clean hands and had repeatedly admitted to the lapse on her part while clicking photos and uploading the PDFs in the said exam.\n",
            "Input token length: 79\n",
            "Output token length: 128\n",
            "hindi:  इलाहाबाद हाईकोर्ट (Supreme Court) ने याचिका की अनुमति दी, हालांकि, कोर्ट ने विचार किया कि यह उसकी इच्छा को स्थापित करने के लिए नहीं हो सकता है, लेकिन वह इस बात से सहमत थी कि इसके पास कोई वकील को अपना बुनियादीपन को साबित नहीं कर सकती है। यद्यपि, अदालत ने तर्क दिया कि पीड़ित की कल्पनाओं में नहीं है कि उसे अपनी स्वतंत्रता\n",
            "------------------------------------------------------------------------\n",
            "\"The Court is also mindful of the fact that due to the advent of COVID-19 pandemic, the structure of examinations has certainly undergone a monumental change, rendering both students and educational institutions alike, struggling to cope-up with the challenges posed by a complete shift to the virtual world.\n",
            "Input token length: 75\n",
            "Output token length: 126\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने कहा, \" COVID-19 महामारी के संकट के मद्देनजर, परीक्षाओं की तैयारी के सिस्टम में वास्तव में कई महत्वपूर्ण  बदलाव हो चुका है। \" अदालत इस बात पर भी ध्यान देने योग्य है कि संक्रमण से जुड़े छात्रों और शैक्षणिक संस्थानों के सभी सदस्यों को पूरी तरह से परेशान करने की आवश्यकता होती है।\" कोर्ट इस तथ्य को यह भी मान\n",
            "------------------------------------------------------------------------\n",
            "The consequences of such an upheaval have deep implications, and the instant petition is just one of the many issues that have been flagged as a by-product of this shift,\" the Court observed.\n",
            "Input token length: 50\n",
            "Output token length: 130\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने कहा, \"इस तरह के टकराव की परिणामों का संदर्भ बहुत महत्वपूर्ण है, और याचिकाकर्ता की अवधि की संभावनाओं का सबसे बड़ा मुद्दा यह है कि ऐसा बदलाव के कई फांसेस हैं। अदालत ने ऐसे मामलों का एक हिस्सा माना जा रहा है। इसके अलावा, इसमें हस्तक्षेप के मुद्दे जैसा किसी भी मुद्दों के एक रूप में ध्यान दिया गया है\n",
            "------------------------------------------------------------------------\n",
            "It was the case of NIFT that the Petitioner was rightly disqualified as she had disclosed the identity of the institute from which she was taking coaching for the exam, which was clearly visible from the PDF answer sheets uploaded by her to the online portal.\n",
            "Input token length: 64\n",
            "Output token length: 128\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने एनआईएफटी (NIFT) के मामले में याचिकाकर्ता को कानूनी रूप से बर्खास्त किया गया क्योंकि उसने नेशनल इंफ्रास्ट्रक्चर सर्विस के संस्थान के नाम को खुद ही रद्द कर दिया था। यह मामला NIFT की मांग की गई थी, जिसमें वह नई इंस्टीट्यूट की डुप्लिकेट प्रश्न पत्रों की PDF जवाब sheets के माध्यम से दिखाया ग\n",
            "------------------------------------------------------------------------\n",
            "Therefore, NIFT had argued that it was a case of her identity being disclosed in the examination, which was in violation of Rule 12 printed on the Admit Card.\n",
            "Input token length: 45\n",
            "Output token length: 130\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने एनआईएफटी को यह कहने के लिए कहा कि परीक्षण में उसकी पहचान को खुलासा किया गया था, जिसकी धारा 12 के तहत प्रिंट कर दी गई थी। इसलिए, NIFT ने इस तर्क पर जोर देने की मांग करने का फैसला सुनाया था। नई दिल्ली में नियोक्ता जांच के दौरान खुद को साबित किए जाने का मामला है। नोटिस डिफॉल्ट ने आगे कहा\n",
            "------------------------------------------------------------------------\n",
            "In this backdrop, the Court was of the view:\n",
            "\"Though the afore-noted pictures reveal the name of the coaching institute, however, they do not indicate the identity of the candidate.\n",
            "Input token length: 49\n",
            "Output token length: 127\n",
            "hindi:  इलाहाबाद हाईकोर्ट (Supreme Court) ने यह ध्यान में रखते हुए कहा कि, हालांकि, उन्हें काउंसिलिंग इंस्टीट्यूट के नाम का खुलासा नहीं कर सकते हैं। इसके अलावा, वे किसी भी उम्मीदवारों की पहचान के रूप में नहीं दिख रहे हैं।\" कोर्ट ने इस दृष्टिकोण से विचार किया गया: \"यद्यपि नीचे दिए गए फोटोज टीम इंटरनेशनल लिमिटेड का नाम स्पष्ट नहीं\n",
            "------------------------------------------------------------------------\n",
            "This identification mark does not seem to violate condition no.\n",
            "Input token length: 15\n",
            "Output token length: 14\n",
            "hindi:  यह पता नहीं लगाया जा सकता है।\n",
            "------------------------------------------------------------------------\n",
            "12 of the Admit Card, and for this reason alone, the Court is inclined to give the benefit of the doubt to the Petitioner.\n",
            "Input token length: 33\n",
            "Output token length: 125\n",
            "hindi:  इलाहाबाद हाईकोर्ट (Supreme Court) ने फैसले में यह टिप् पणी की है कि एडमिड रिकॉर्ड की तारीख 12 के लिए आवेदन दावा करने की अनुमति देने का इरादा रखता है। इसलिए याचिकाकर्ता को इस कारण से संतुष्ट किया जाएगा जब तक कि अदालत उसकी सजा का लाभ लेने में सक्षम नहीं होगा। हालांकि, इसका मतलब के बावजूद,\n",
            "------------------------------------------------------------------------\n",
            "Surely, the identification of a candidate can also be revealed by giving a hint or a suggestion, but that can occur if there is a nexus between the candidate and the examiner.\n",
            "Input token length: 50\n",
            "Output token length: 121\n",
            "hindi:  इलाहाबाद में एक उम्मीदवार की पहचान का खुलासा भी साबित किया जा सकता है, लेकिन ऐसा नहीं होता, यहां तक कि आवेदक और परीक्षक के बीच कोई टक्कर हो सकती है। इसके अलावा, एडमिस्ट्रेटिव नामांकन भी एक अन्य तरीका द्वारा भी प्रदर्शित कर सकते हैं। हालांकि, इसे इस तरह प्रकट करने का भी आरोप भी लगाया गया होगा।\n",
            "------------------------------------------------------------------------\n",
            "However, such a case is not canvassed by the Respondent.\"\n",
            "Input token length: 19\n",
            "Output token length: 39\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने कहा, \"इस मामले में प्रतिवादी द्वारा जवाबदेह के रूप में विचार नहीं किया जा सकता है।\n",
            "------------------------------------------------------------------------\n",
            "The Court also noted that there was no allegation against the Petitioner candidate of any malpractice or use of unfair means and that she was required to analyze the question and draw her answers on the questions-cum-answer sheets.\n",
            "Input token length: 55\n",
            "Output token length: 124\n",
            "hindi:  इलाहाबाद हाईकोर्ट (Supreme Court) ने यह भी नोट किया कि याचिकाकर्ता के खिलाफ किसी भी भ्रष्टाचार या गैरकानूनी तरीकों का उपयोग करने के आरोप में कोई शिकायत नहीं की गई थी। कोर्ट ने आगे कहा कि अदालत उसकी सहायता के लिए निर्देश नहीं दिया गया है और उसे प्रश्न-आ-जवाब शीट पर रखे गए थे, जिसने अपने उत्तरों का विश्ले\n",
            "------------------------------------------------------------------------\n",
            "The Court also noted that the petitioner was then required to click pictures thereof, stitch them together into PDF files, and upload them on the portal, all within the stipulated time.\n",
            "Input token length: 42\n",
            "Output token length: 128\n",
            "hindi:  इलाहाबाद हाईकोर्ट (Supreme Court) ने कहा कि याचिकाकर्ता को उसकी शीर्षक की कटौती करने का निर्देश दिया गया था। कोर्ट ने यह भी नोट किया कि पीड़ित को उचित समय के भीतर प्रिฟॉपिक बनाने की आवश्यकता होगी। अदालत ने इस बात पर जोर देने के लिए कहा है कि उन्होंने वकील को वेबसाइट पर क्लिक करें और उन्हें पीडीएफ फाइल में रखा जाए\n",
            "------------------------------------------------------------------------\n",
            "\"It cannot be forgotten that the Petitioner is just a teenager, and given the enormity of appearing for an entrance exam which decides her career, coupled with the obligations of a new-format online system, it could have taken a toll on the Petitioner who was in a pinch of time.\n",
            "Input token length: 82\n",
            "Output token length: 124\n",
            "hindi:  यह ध्यान नहीं दिया जा सकता है कि याचिकाकर्ता एक नाबालिग है, और इसके साथ ही उसकी पढ़ाई-लिखने की संभावना एक युवा लड़की के रूप में होती है। \"यह कोई संकोचित नहीं होगा कि पिछले साल के एक छात्र परीक्षा पूरी तरह से बच्चे की उम्र को निर्धारित करने का संकल्प किया गया है।\" \"इस बात को याद रखने को नहीं भू\n",
            "------------------------------------------------------------------------\n",
            "It is therefore plausible that due to paucity of time, towards the fag end of the exam, the Petitioner panicked and did not crop or properly click photographs of her answer sheets and hurriedly uploaded the same to conform with the deadline,\" it said.\n",
            "Input token length: 64\n",
            "Output token length: 127\n",
            "hindi:  इलाहाबाद हाईकोर्ट ने एक फैसले में कहा कि परीक्षा समाप्त होने के कारण, प्रतियोगी को समय के भीतर प्रश्न पत्रों की कटौती के दौरान, \"इस प्रकार माना जा सकता है कि चतुर्थता की अवधि के बावजूद, उसकी प्रश्न शीटों को ठीक से ढूढ़ कर लोड करने में विफल रही थी। इसलिए यह तथ्य भविष्यवाणी की जानी चाहिए कि निश्चित रूप से\n",
            "------------------------------------------------------------------------\n",
            "The Court added \"In view of the foregoing, the Court is of the view that a momentary lapse on the part of the candidate must not be met with such a severe punitive action, which would cause grave and irreparable prejudice and affect the Petitioner's future perversely.\"\n",
            "Input token length: 71\n",
            "Output token length: 129\n",
            "hindi:  इलाहाबाद हाईकोर्ट (Supreme Court) ने कहा, \"आगे आए मामलों की निष्कर्ष पर विचार करते हुए, न्यायालय इस दृष्टिकोण से यह ध्यान देने योग्य है कि एक उम्मीदवार की ओर से किसी भी क्षणिक देरी को मध्यस्थता के रूप में नहीं मानी जानी चाहिए। इसके साथ ही अदालत का फैसला सुनिश्चित करने का निर्देश दिया गया है, ताकि इस तरह की गंभीर दंडात्मक कार्र\n",
            "------------------------------------------------------------------------\n",
            "The plea was accordingly allowed.\n",
            "Input token length: 11\n",
            "Output token length: 10\n",
            "hindi:  सुप्रीम कोर्ट (Supreme Court)\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MugyIUMTL8YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removes repeated words"
      ],
      "metadata": {
        "id": "AM6hEquT8Ig5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Kb54Y_309b1u",
        "outputId": "f791b20a-4298-41a3-c32e-7e0003b5ddc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। नाबालिग की गर्भपात की रोक लगाने की अनुमति दी गई थी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_repeated_phrases(text):\n",
        "    sentences = text.split(\"।\")  # Splitting sentences for better phrase detection\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        words = sentence.split()\n",
        "        cleaned_words = []\n",
        "        seen_phrases = set()\n",
        "\n",
        "        j = 0\n",
        "        while j < len(words):\n",
        "            phrase = words[j]\n",
        "            k = j + 1\n",
        "\n",
        "            while k < len(words) and ' '.join(words[j:k+1]) in text:\n",
        "                phrase = ' '.join(words[j:k+1])\n",
        "                k += 1\n",
        "\n",
        "            if phrase not in seen_phrases:\n",
        "                cleaned_words.append(phrase)\n",
        "                seen_phrases.add(phrase)\n",
        "\n",
        "            j = k\n",
        "\n",
        "        cleaned_sentence = ' '.join(cleaned_words)\n",
        "\n",
        "        # Check if the sentence is mostly repeated from a previous one\n",
        "        is_duplicate = any(\n",
        "            len(set(cleaned_sentence.split()) & set(prev_sentence.split())) / len(set(cleaned_sentence.split())) > 0.7\n",
        "            for prev_sentence in cleaned_sentences\n",
        "        )\n",
        "\n",
        "        if not is_duplicate:\n",
        "            cleaned_sentences.append(cleaned_sentence)\n",
        "\n",
        "    return '। '.join(cleaned_sentences).strip()\n",
        "\n",
        "# Example usage:\n",
        "text1 = \"my name name name is xyz\"\n",
        "text2 = \"there is a school a school a school bus\"\n",
        "text3 = \"यह एक स्कूल स्कूल स्कूल बस है\"\n",
        "text4 = \"बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। नाबालिग की गर्भपात की रोक लगाने की अनुमति दी गई थी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी\"\n",
        "\n",
        "\n",
        "out1 = remove_repeated_phrases2(text1)\n",
        "print( out1 )\n",
        "print(remove_repeated_phrases(   out1  ))  # Output: \"my name is xyz\"\n",
        "# print(remove_repeated_phrases(   remove_repeated_phrases2(text2)   ))  # Output: \"there is a school bus\"\n",
        "# print(remove_repeated_phrases(   remove_repeated_phrases2(text3)    ))  # Output: \"यह एक स्कूल बस है\"\n",
        "# print(remove_repeated_phrases(   remove_repeated_phrases2(text4   )) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXnclT0L8Jmh",
        "outputId": "b12583ed-0ff3-4c80-ca0d-6d1c803c268e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name name name is xyz\n",
            "my name name name is xyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_repeated_phrases2(text):\n",
        "    def remove_repeated_words(sentence):\n",
        "        words = sentence.split()\n",
        "        cleaned_words = []\n",
        "        i = 0\n",
        "        while i < len(words):\n",
        "            phrase = words[i]\n",
        "            j = i + 1\n",
        "            while j < len(words) and ' '.join(words[i:j+1]) in sentence:\n",
        "                phrase = ' '.join(words[i:j+1])\n",
        "                j += 1\n",
        "            if not cleaned_words or phrase != cleaned_words[-1]:\n",
        "                cleaned_words.append(phrase)\n",
        "            i = j\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    sentences = text.split(\"।\")  # Splitting sentences for better phrase detection\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        cleaned_sentence = remove_repeated_words(sentence.strip())\n",
        "\n",
        "        # Check if the sentence is mostly repeated from a previous one\n",
        "        is_duplicate = any(\n",
        "            len(set(cleaned_sentence.split()) & set(prev_sentence.split())) / len(set(cleaned_sentence.split())) > 0.7\n",
        "            for prev_sentence in cleaned_sentences\n",
        "        )\n",
        "\n",
        "        if not is_duplicate:\n",
        "            cleaned_sentences.append(cleaned_sentence)\n",
        "\n",
        "    return '। '.join(cleaned_sentences).strip()\n",
        "\n",
        "# Example usage:\n",
        "text1 = \"my name name name is xyz\"\n",
        "text2 = \"there is a school a school a school bus\"\n",
        "text3 = \"यह एक स्कूल स्कूल स्कूल बस है\"\n",
        "text4 = \"बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। नाबालिग की गर्भपात की रोक लगाने की अनुमति दी गई थी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी\"\n",
        "\n",
        "print(remove_repeated_phrases2(text1))  # Output: \"my name is xyz\"\n",
        "print(remove_repeated_phrases2(text2))  # Output: \"there is a school bus\"\n",
        "print(remove_repeated_phrases2(text3))  # Output: \"यह एक स्कूल बस है\"\n",
        "print(remove_repeated_phrases2(text4))\n",
        "\n",
        "\n",
        "#   # Example usage:\n",
        "# text1 = \"my name name name is xyz\"\n",
        "# text2 = \"there is a school a school a school bus\"\n",
        "# text3 = \"यह एक स्कूल स्कूल स्कूल बस है\"\n",
        "# text4 = \"बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। नाबालिग की गर्भपात की रोक लगाने की अनुमति दी गई थी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी\"\n",
        "\n",
        "# remove_repeated_phrases2(text1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSQ1qGUg8KCl",
        "outputId": "bf22cab3-cc90-4360-8d6c-79b5487e4043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name name name is xyz\n",
            "there is a school a school a school bus\n",
            "यह एक स्कूल स्कूल स्कूल बस है\n",
            "बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_repeated_phrases(text):\n",
        "    words = text.split()\n",
        "    cleaned_words = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        phrase = words[i]\n",
        "\n",
        "        # Check for repeated sequences of words\n",
        "        match = re.match(rf'({re.escape(phrase)}(?:\\s+{re.escape(phrase)})+)', ' '.join(words[i:]))\n",
        "\n",
        "        if match:\n",
        "            cleaned_words.append(phrase)\n",
        "            i += match.group(0).count(phrase)  # Skip repeated occurrences\n",
        "        else:\n",
        "            cleaned_words.append(phrase)\n",
        "            i += 1\n",
        "\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "text1 = \"my name name name is xyz\"\n",
        "text2 = \"there is a school a school a school bus\"\n",
        "text3 = \"यह एक स्कूल स्कूल स्कूल बस है\"\n",
        "text4 = \"बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। नाबालिग की गर्भपात की रोक लगाने की अनुमति दी गई थी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी\"\n",
        "\n",
        "print(remove_repeated_phrases(text1))  # Output: \"my name is xyz\"\n",
        "print(remove_repeated_phrases(text2))  # Output: \"there is a school bus\"\n",
        "print(remove_repeated_phrases(text3))  # Output: \"यह एक स्कूल बस है\"\n",
        "print(remove_repeated_phrases(text4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q60ugb7_Hpc",
        "outputId": "5232b25f-ef40-4b19-d5ed-a5301ef06649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name is xyz\n",
            "there is a school a school a school bus\n",
            "यह एक स्कूल बस है\n",
            "बॉम्बे हाईकोर्ट ने हाल ही में एक नाबालिग की गर्भपात की रोक लगाने की अनुमति दी। नाबालिग की गर्भपात की रोक लगाने की अनुमति दी गई थी। जस्टिस एएस चंडूरकर और जस्टिस उर्मिला जोशी-फाल्के की पीठ ने कहा कि सुप्रीम कोर्ट ने माना है कि गर्भपात की स्वतंत्रता एक महिला के व्यक्तिगत स्वतंत्रता का एक अनिवार्य हिस्सा है। जस्टिस अश्विनी\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNlqkDFuFjBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}